{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-crfsuite in c:\\users\\aleix\\anaconda3\\lib\\site-packages (0.9.5)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ALEIX\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ALEIX\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from time import time\n",
    "import collections\n",
    "# from drug_entity_chunker_v2 import ScikitLearnChunker\n",
    "from word2features import *\n",
    "from nltk import download\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# from nltk import word_tokenize\n",
    "from parser_utils_v2 import  parse_drug_iob_xy\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import numpy as np\n",
    "# from nltk import tree2conlltags\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.externals import joblib\n",
    "\n",
    "# from itertools import chain\n",
    "import nltk\n",
    "import sklearn\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/ALEIX/Documents/GitHub/MAI_AHLT_DDIproject/python-crfsuite-master\")\n",
    "import pycrfsuite\n",
    "!pip install python-crfsuite\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "# NLTK dependencies\n",
    "download('punkt')\n",
    "download('averaged_perceptron_tagger')\n",
    "\n",
    "# Directories to find MedLine and DrugBank train and test data\n",
    "DRUGBANK_TRAIN_DIR = './Train/DrugBank'\n",
    "DRUGBANK_TEST_DIR = './Test/Test for DrugNER task/DrugBank'\n",
    "MEDLINE_TRAIN_DIR = './Train/MedLine'\n",
    "MEDLINE_TEST_DIR = './Test/Test for DrugNER task/MedLine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DRUGBANK train data...\n",
      "done in 148.348941s \n",
      "\n",
      "Loading DRUGBANK test data...\n",
      "done in 3.975694s \n",
      "\n",
      "train instances:  119978 test instances:  2929\n",
      "\n",
      " Loading MEDLINE train data...\n",
      "done in 36.055856s \n",
      "\n",
      "Loading MEDLINE test data...\n",
      "done in 14.718377s \n",
      "\n",
      "train instances:  31187 test instances:  12607\n"
     ]
    }
   ],
   "source": [
    "tags = ['drug', 'brand', 'group', 'drug_n']\n",
    "\n",
    "print(\"Loading DRUGBANK train data...\")\n",
    "t0 = time()\n",
    "\n",
    "drugb_x_train_dic, drugb_y_train_dic, _, _ = parse_drug_iob_xy(DRUGBANK_TRAIN_DIR, dict_features, tags=tags)\n",
    "drugb_x_train_arr, drugb_y_train_arr, _, _ = parse_drug_iob_xy(DRUGBANK_TRAIN_DIR, array_features, tags=tags)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "print(\"Loading DRUGBANK test data...\")\n",
    "t0 = time()\n",
    "drugb_x_test_dic, drugb_y_test_dic, _, _ = parse_drug_iob_xy(DRUGBANK_TEST_DIR, dict_features, tags=tags)\n",
    "drugb_x_test_arr, drugb_y_test_arr, drugb_sId_test, drugb_words_start = parse_drug_iob_xy(DRUGBANK_TEST_DIR, array_features, tags=tags)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "print(\"train instances: \", len(drugb_x_train_dic), \"test instances: \", len(drugb_x_test_dic))\n",
    "\n",
    "print(\"\\n Loading MEDLINE train data...\")\n",
    "t0 = time()\n",
    "\n",
    "med_x_train_dic, med_y_train_dic, _, _ = parse_drug_iob_xy(MEDLINE_TRAIN_DIR, dict_features, tags=tags)\n",
    "med_x_train_arr, med_y_train_arr, _, _ = parse_drug_iob_xy(MEDLINE_TRAIN_DIR, array_features, tags=tags)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "print(\"Loading MEDLINE test data...\")\n",
    "t0 = time()\n",
    "med_x_test_dic, med_y_test_dic, _, _ = parse_drug_iob_xy(MEDLINE_TEST_DIR, dict_features, tags=tags)\n",
    "med_x_test_arr, med_y_test_arr, med_sId_test, med_words_start = parse_drug_iob_xy(MEDLINE_TEST_DIR, array_features, tags=tags)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "print(\"train instances: \", len(med_x_train_dic), \"test instances: \", len(med_x_test_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA SKLEARN MODELS\n",
      "Loading DRUGBANK train data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ec777db4d4a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading DRUGBANK train data...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdrugb_x_train_dic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrugb_y_train_dic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_drug_iob_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDRUGBANK_TRAIN_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mduration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"done in %fs \\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mduration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\MAI_AHLT_DDIproject\\parser_utils_v2.py\u001b[0m in \u001b[0;36mparse_drug_iob_xy\u001b[1;34m(xml_dir, feature_detector, tags)\u001b[0m\n\u001b[0;32m    108\u001b[0m                     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_from_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                     \u001b[0mdrug_positions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdrug_pos_from_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_tags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                     \u001b[0mtagged_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_bio_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrug_positions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged_text\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\MAI_AHLT_DDIproject\\parser_utils_v2.py\u001b[0m in \u001b[0;36mpos_bio_tagger\u001b[1;34m(text, drug_positions)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpos_bio_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrug_positions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mpos_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[0mextended_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mfind_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \"\"\"\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[0mAP_MODEL_LOC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'file:'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'taggers/averaged_perceptron_tagger/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mPICKLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;31m# Is the path item a directory or is resource_name an absolute path?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mzipfile\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m                 \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl2pathname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tags = ['drug', 'brand', 'group', 'drug_n']\n",
    "\n",
    "print(\"DATA SKLEARN MODELS\")\n",
    "print(\"Loading DRUGBANK train data...\")\n",
    "t0 = time()\n",
    "drugb_x_train_dic, drugb_y_train_dic, _, _ = parse_drug_iob_xy(DRUGBANK_TRAIN_DIR, dict_features, tags=tags)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "print(\"Loading DRUGBANK test data...\")\n",
    "t0 = time()\n",
    "drugb_x_test_dic, drugb_y_test_dic, _, _ = parse_drug_iob_xy(DRUGBANK_TEST_DIR, dict_features, tags=tags)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "print(\"train instances: \", len(drugb_x_train_dic), \"test instances: \", len(drugb_x_test_dic))\n",
    "\n",
    "print(\"\\n Loading MEDLINE train data...\")\n",
    "t0 = time()\n",
    "med_x_train_dic, med_y_train_dic, _, _ = parse_drug_iob_xy(MEDLINE_TRAIN_DIR, dict_features, tags=tags)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "print(\"Loading MEDLINE test data...\")\n",
    "t0 = time()\n",
    "med_x_test_dic, med_y_test_dic, _, _ = parse_drug_iob_xy(MEDLINE_TEST_DIR, dict_features, tags=tags)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "print(\"train instances: \", len(med_x_train_dic), \"test instances: \", len(med_x_test_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapting DRUGBANK data to numeric format\n",
      "done in 4.112657s \n",
      "\n",
      "Adapting MEDLINE data to numeric format\n",
      "done in 1.202290s \n",
      "\n",
      "Unifying both datasets data\n",
      "done in 1.203290s\n"
     ]
    }
   ],
   "source": [
    "classes = ['O', 'B-drug', 'I-drug', 'B-brand', 'I-brand', 'B-group', 'I-group', 'B-drug_n', 'I-drug_n']\n",
    "\n",
    "print(\"Adapting DRUGBANK data to numeric format\")\n",
    "t0 = time()\n",
    "vectorizer = DictVectorizer(sparse=True)\n",
    "drugb_x_train_dic_vec = vectorizer.fit_transform(drugb_x_train_dic)\n",
    "drugb_x_test_dic_vec = vectorizer.transform(drugb_x_test_dic)\n",
    "\n",
    "drugb_y_train_dic_vec = vectorize_attr(drugb_y_train_dic, classes)   \n",
    "drugb_y_test_dic_vec = vectorize_attr(drugb_y_test_dic, classes)\n",
    "\n",
    "drugb_y_test_arr_vec = vectorize_attr(drugb_y_test_arr, classes)\n",
    "\n",
    "duration = time() - t0\n",
    "print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "print(\"Adapting MEDLINE data to numeric format\")\n",
    "t0 = time()\n",
    "med_x_train_dic_vec = vectorizer.transform(med_x_train_dic)\n",
    "med_x_test_dic_vec = vectorizer.transform(med_x_test_dic)\n",
    "\n",
    "med_y_train_dic_vec = vectorize_attr(med_y_train_dic, classes)   \n",
    "med_y_test_dic_vec = vectorize_attr(med_y_test_dic, classes)\n",
    "\n",
    "med_y_test_arr_vec = vectorize_attr(med_y_test_arr, classes)\n",
    "\n",
    "duration = time() - t0; print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "print(\"Unifying both datasets data\")\n",
    "datasetNames = ['DrugBank', 'MedLine']\n",
    "x_train_dic_vec = [drugb_x_train_dic_vec, med_x_train_dic_vec]\n",
    "y_train_dic_vec = [drugb_y_train_dic_vec, med_y_train_dic_vec]\n",
    "\n",
    "x_test_dic_vec = [drugb_x_test_dic_vec, med_x_test_dic_vec]\n",
    "y_test_dic_vec = [drugb_y_test_dic_vec, med_y_test_dic_vec]\n",
    "\n",
    "y_test_arr_vec = [drugb_y_test_arr_vec, med_y_test_arr_vec]\n",
    "\n",
    "x_train_arr = [drugb_x_train_arr, med_x_train_arr]\n",
    "y_train_arr = [drugb_y_train_arr, med_y_train_arr]\n",
    "\n",
    "x_test_arr = [drugb_x_test_arr, med_x_test_arr]\n",
    "y_test_arr = [drugb_y_test_arr, med_y_test_arr]\n",
    "\n",
    "duration = time() - t0; print(\"done in %fs\" % (duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapting DRUGBANK data to numeric format\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'drugb_y_test_arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f070cf7452df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# drugb_y_test_dic_vec = vectorize_attr(drugb_y_test_dic, classes)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdrugb_y_test_arr_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorize_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrugb_y_test_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mduration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'drugb_y_test_arr' is not defined"
     ]
    }
   ],
   "source": [
    "classes = ['O', 'B-drug', 'I-drug', 'B-brand', 'I-brand', 'B-group', 'I-group', 'B-drug_n', 'I-drug_n']\n",
    "\n",
    "print(\"Adapting DRUGBANK data to numeric format\"); t0 = time()\n",
    "vectorizer = DictVectorizer(sparse=True)\n",
    "drugb_x_train_dic_vec = vectorizer.fit_transform(drugb_x_train_dic)\n",
    "drugb_x_test_dic_vec = vectorizer.transform(drugb_x_test_dic)\n",
    "\n",
    "drugb_y_train_dic_vec = vectorize_attr(drugb_y_train_dic, classes)   \n",
    "drugb_y_test_dic_vec = vectorize_attr(drugb_y_test_dic, classes)\n",
    "\n",
    "duration = time() - t0; print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "print(\"Adapting MEDLINE data to numeric format\"); t0 = time()\n",
    "med_x_train_dic_vec = vectorizer.transform(med_x_train_dic)\n",
    "med_x_test_dic_vec = vectorizer.transform(med_x_test_dic)\n",
    "\n",
    "med_y_train_dic_vec = vectorize_attr(med_y_train_dic, classes)   \n",
    "med_y_test_dic_vec = vectorize_attr(med_y_test_dic, classes)\n",
    "duration = time() - t0; print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "print(\"Unifying both datasets data\")\n",
    "datasetNames = ['DrugBank', 'MedLine']\n",
    "x_train_dic_vec = [drugb_x_train_dic_vec, med_x_train_dic_vec]\n",
    "y_train_dic_vec = [drugb_y_train_dic_vec, med_y_train_dic_vec]\n",
    "\n",
    "x_test_dic_vec = [drugb_x_test_dic_vec, med_x_test_dic_vec]\n",
    "y_test_dic_vec = [drugb_y_test_dic_vec, med_y_test_dic_vec]\n",
    "duration = time() - t0; print(\"done in %fs\" % (duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define classifier models\n",
    "clf_perceptron5 = Perceptron(verbose=0, n_jobs=-1, max_iter=5)\n",
    "clf_perceptron50 = Perceptron(verbose=0, n_jobs=-1, max_iter=50)\n",
    "clf_perceptron100 = Perceptron(verbose=0, n_jobs=-1, max_iter=100)\n",
    "clf_tree = DecisionTreeClassifier(criterion='entropy')\n",
    "clf_NB = MultinomialNB()\n",
    "clf_MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100), random_state=1)\n",
    "\n",
    "#  (RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
    "#         (Perceptron(n_iter=50), \"Perceptron\"),\n",
    "#         (PassiveAggressiveClassifier(n_iter=50), \"Passive-Aggressive\"),\n",
    "#         (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "#         (RandomForestClassifier(n_estimators=100), \"Random forest\")):\n",
    "\n",
    "# for penalty in [\"l2\", \"l1\"]:\n",
    "#     print('=' * 80)\n",
    "#     print(\"%s penalty\" % penalty.upper())\n",
    "#     # Train Liblinear model\n",
    "#     results.append(benchmark(LinearSVC(penalty=penalty, dual=False,\n",
    "#                                        tol=1e-3)))\n",
    "\n",
    "#     # Train SGD model\n",
    "#     results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "#                                            penalty=penalty)))\n",
    "# LinearSVC(penalty=\"l2\"))\n",
    "# http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html\n",
    "\n",
    "names = [\"Perceptron5 \", \"Perceptron 50\", \"Perceptron 100\",\"Decision tree\", \"Naive Bayes\", \"Multi Layer Perceptron\"]\n",
    "clfs = [clf_perceptron5, clf_perceptron50, clf_perceptron100, clf_tree, clf_NB, clf_MLP]\n",
    "\n",
    "print(\"Sklearn models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifiers\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Perceptron5  with DrugBank dataset**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Testing...\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.98      0.98      0.98      2569\n",
      "     B-drug       0.87      0.79      0.83       179\n",
      "     I-drug       0.76      0.65      0.70        20\n",
      "    B-brand       0.71      0.74      0.72        53\n",
      "    I-brand       0.33      1.00      0.50         1\n",
      "    B-group       0.83      0.83      0.83        64\n",
      "    I-group       0.83      0.94      0.88        31\n",
      "   B-drug_n       1.00      0.17      0.29         6\n",
      "   I-drug_n       1.00      0.83      0.91         6\n",
      "\n",
      "avg / total       0.96      0.96      0.96      2929\n",
      "\n",
      "\n",
      " CV scores:  [ 0.9220339   0.93515358  0.89931741  0.92478632  0.90893471] , mean:  0.918045184151 \n",
      "\n",
      "Confusion matrix\n",
      "[[2530   13    2   13    1    5    5    0    0]\n",
      " [  27  141    2    3    1    5    0    0    0]\n",
      " [   6    1   13    0    0    0    0    0    0]\n",
      " [   8    5    0   39    0    1    0    0    0]\n",
      " [   0    0    0    0    1    0    0    0    0]\n",
      " [  10    0    0    0    0   53    1    0    0]\n",
      " [   2    0    0    0    0    0   29    0    0]\n",
      " [   3    2    0    0    0    0    0    1    0]\n",
      " [   1    0    0    0    0    0    0    0    5]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Perceptron5  with MedLine dataset**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Testing...\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.98      0.99      0.98     12033\n",
      "     B-drug       0.60      0.33      0.43       168\n",
      "     I-drug       0.86      0.64      0.73        28\n",
      "    B-brand       0.00      0.00      0.00         6\n",
      "    I-brand       0.00      0.00      0.00         0\n",
      "    B-group       0.73      0.36      0.48        90\n",
      "    I-group       0.68      0.93      0.78        58\n",
      "   B-drug_n       0.25      0.18      0.21       114\n",
      "   I-drug_n       0.88      0.84      0.86       110\n",
      "\n",
      "avg / total       0.96      0.97      0.96     12607\n",
      "\n",
      "\n",
      " CV scores:  [ 0.96315372  0.96987713  0.96195006  0.9511711   0.96544877] , mean:  0.962320156521 \n",
      "\n",
      "Confusion matrix\n",
      "[[11917    23     3     0     3     9    22    45    11]\n",
      " [   99    56     0     0     0     1     0    12     0]\n",
      " [    8     0    18     0     0     0     0     0     2]\n",
      " [    4     1     0     0     0     1     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0]\n",
      " [   50     3     0     0     1    32     0     4     0]\n",
      " [    4     0     0     0     0     0    54     0     0]\n",
      " [   82    11     0     0     0     1     0    20     0]\n",
      " [   13     0     0     0     1     0     4     0    92]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Perceptron 50 with DrugBank dataset**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Testing...\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.98      0.99      0.99      2569\n",
      "     B-drug       0.90      0.81      0.85       179\n",
      "     I-drug       0.85      0.85      0.85        20\n",
      "    B-brand       0.83      0.72      0.77        53\n",
      "    I-brand       0.50      1.00      0.67         1\n",
      "    B-group       0.76      0.88      0.81        64\n",
      "    I-group       0.86      1.00      0.93        31\n",
      "   B-drug_n       0.33      0.17      0.22         6\n",
      "   I-drug_n       1.00      1.00      1.00         6\n",
      "\n",
      "avg / total       0.97      0.97      0.97      2929\n",
      "\n",
      "\n",
      " CV scores:  [ 0.92033898  0.94368601  0.91296928  0.92991453  0.91408935] , mean:  0.924199630029 \n",
      "\n",
      "Confusion matrix\n",
      "[[2536    9    0    7    1   12    4    0    0]\n",
      " [  23  145    3    1    0    5    0    2    0]\n",
      " [   3    0   17    0    0    0    0    0    0]\n",
      " [  10    5    0   38    0    0    0    0    0]\n",
      " [   0    0    0    0    1    0    0    0    0]\n",
      " [   6    1    0    0    0   56    1    0    0]\n",
      " [   0    0    0    0    0    0   31    0    0]\n",
      " [   2    2    0    0    0    1    0    1    0]\n",
      " [   0    0    0    0    0    0    0    0    6]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Perceptron 50 with MedLine dataset**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Testing...\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.98      0.99      0.99     12033\n",
      "     B-drug       0.62      0.48      0.54       168\n",
      "     I-drug       0.89      0.57      0.70        28\n",
      "    B-brand       0.00      0.00      0.00         6\n",
      "    I-brand       0.00      0.00      0.00         0\n",
      "    B-group       0.64      0.30      0.41        90\n",
      "    I-group       0.86      0.86      0.86        58\n",
      "   B-drug_n       0.18      0.04      0.07       114\n",
      "   I-drug_n       0.91      0.61      0.73       110\n",
      "\n",
      "avg / total       0.96      0.97      0.96     12607\n",
      "\n",
      "\n",
      " CV scores:  [ 0.96156894  0.96274277  0.96234641  0.95553791  0.96743447] , mean:  0.961926100283 \n",
      "\n",
      "Confusion matrix\n",
      "[[11963    26     1     1     3    11     8    15     5]\n",
      " [   80    80     0     0     0     0     0     8     0]\n",
      " [   10     0    16     0     0     0     0     0     2]\n",
      " [    5     0     0     0     0     1     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0]\n",
      " [   57     6     0     0     0    27     0     0     0]\n",
      " [    8     0     0     0     0     0    50     0     0]\n",
      " [   91    17     0     0     0     1     0     5     0]\n",
      " [   37     1     1     0     2     2     0     0    67]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Perceptron 100 with DrugBank dataset**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Testing...\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.98      0.98      0.98      2569\n",
      "     B-drug       0.85      0.78      0.81       179\n",
      "     I-drug       0.75      0.90      0.82        20\n",
      "    B-brand       0.76      0.70      0.73        53\n",
      "    I-brand       0.14      1.00      0.25         1\n",
      "    B-group       0.74      0.88      0.80        64\n",
      "    I-group       0.89      1.00      0.94        31\n",
      "   B-drug_n       0.50      0.17      0.25         6\n",
      "   I-drug_n       1.00      1.00      1.00         6\n",
      "\n",
      "avg / total       0.96      0.96      0.96      2929\n",
      "\n",
      "\n",
      " CV scores:  [ 0.92033898  0.94368601  0.91296928  0.92991453  0.91408935] , mean:  0.924199630029 \n",
      "\n",
      "Confusion matrix\n",
      "[[2526   14    3    5    5   12    4    0    0]\n",
      " [  22  139    3    7    0    7    0    1    0]\n",
      " [   2    0   18    0    0    0    0    0    0]\n",
      " [   9    7    0   37    0    0    0    0    0]\n",
      " [   0    0    0    0    1    0    0    0    0]\n",
      " [   6    1    0    0    1   56    0    0    0]\n",
      " [   0    0    0    0    0    0   31    0    0]\n",
      " [   2    2    0    0    0    1    0    1    0]\n",
      " [   0    0    0    0    0    0    0    0    6]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Perceptron 100 with MedLine dataset**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Testing...\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.98      0.97      0.98     12033\n",
      "     B-drug       0.49      0.49      0.49       168\n",
      "     I-drug       0.63      0.61      0.62        28\n",
      "    B-brand       0.00      0.00      0.00         6\n",
      "    I-brand       0.00      0.00      0.00         0\n",
      "    B-group       0.52      0.43      0.47        90\n",
      "    I-group       0.78      0.91      0.84        58\n",
      "   B-drug_n       0.17      0.41      0.24       114\n",
      "   I-drug_n       0.84      0.67      0.75       110\n",
      "\n",
      "avg / total       0.96      0.95      0.96     12607\n",
      "\n",
      "\n",
      " CV scores:  [ 0.95998415  0.96789536  0.96155371  0.95394998  0.96386021] , mean:  0.961448681475 \n",
      "\n",
      "Confusion matrix\n",
      "[[11699    67     8     1    10    31    13   192    12]\n",
      " [   50    83     1     0     1     1     0    32     0]\n",
      " [    8     0    17     0     1     0     0     0     2]\n",
      " [    4     0     0     0     0     1     0     1     0]\n",
      " [    0     0     0     0     0     0     0     0     0]\n",
      " [   35     5     0     0     7    39     0     4     0]\n",
      " [    5     0     0     0     0     0    53     0     0]\n",
      " [   53    12     0     0     1     1     0    47     0]\n",
      " [   27     1     1     0     3     2     2     0    74]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Decision tree with DrugBank dataset**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Testing...\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.96      0.98      0.97      2569\n",
      "     B-drug       0.76      0.69      0.72       179\n",
      "     I-drug       0.76      0.65      0.70        20\n",
      "    B-brand       0.71      0.79      0.75        53\n",
      "    I-brand       0.50      1.00      0.67         1\n",
      "    B-group       0.67      0.55      0.60        64\n",
      "    I-group       0.88      0.94      0.91        31\n",
      "   B-drug_n       0.00      0.00      0.00         6\n",
      "   I-drug_n       1.00      0.83      0.91         6\n",
      "\n",
      "avg / total       0.94      0.94      0.94      2929\n",
      "\n",
      "\n",
      " CV scores:  [ 0.89661017  0.91467577  0.89249147  0.91282051  0.90721649] , mean:  0.90476288253 \n",
      "\n",
      "Confusion matrix\n",
      "[[2506   31    3   12    1   12    4    0    0]\n",
      " [  46  123    1    4    0    5    0    0    0]\n",
      " [   7    0   13    0    0    0    0    0    0]\n",
      " [  11    0    0   42    0    0    0    0    0]\n",
      " [   0    0    0    0    1    0    0    0    0]\n",
      " [  23    5    0    1    0   35    0    0    0]\n",
      " [   2    0    0    0    0    0   29    0    0]\n",
      " [   3    3    0    0    0    0    0    0    0]\n",
      " [   1    0    0    0    0    0    0    0    5]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Decision tree with MedLine dataset**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Testing...\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.98      0.98      0.98     12033\n",
      "     B-drug       0.28      0.40      0.33       168\n",
      "     I-drug       0.67      0.50      0.57        28\n",
      "    B-brand       0.00      0.00      0.00         6\n",
      "    I-brand       0.38      0.19      0.25        90\n",
      "    B-group       0.65      0.86      0.74        58\n",
      "    I-group       0.19      0.15      0.17       114\n",
      "   B-drug_n       0.84      0.81      0.82       110\n",
      "\n",
      "avg / total       0.95      0.95      0.95     12607\n",
      "\n",
      "\n",
      " CV scores:  [ 0.95839937  0.96432818  0.96749901  0.95434696  0.95631454] , mean:  0.960177610873 \n",
      "\n",
      "Confusion matrix\n",
      "[[11756   148     7     1    26    26    55    14]\n",
      " [   86    67     0     0     0     0    15     0]\n",
      " [   11     0    14     0     0     0     0     3]\n",
      " [    5     0     0     0     1     0     0     0]\n",
      " [   65     6     0     0    17     1     1     0]\n",
      " [    7     0     0     0     0    50     1     0]\n",
      " [   73    22     0     1     1     0    17     0]\n",
      " [   21     0     0     0     0     0     0    89]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Naive Bayes with DrugBank dataset**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Testing...\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      0.95      0.96      2569\n",
      "     B-drug       0.48      0.86      0.62       179\n",
      "     I-drug       1.00      0.15      0.26        20\n",
      "    B-brand       0.00      0.00      0.00        53\n",
      "    I-brand       0.00      0.00      0.00         1\n",
      "    B-group       0.73      0.72      0.72        64\n",
      "    I-group       0.81      0.84      0.83        31\n",
      "   B-drug_n       0.00      0.00      0.00         6\n",
      "   I-drug_n       0.00      0.00      0.00         6\n",
      "\n",
      "avg / total       0.92      0.91      0.91      2929\n",
      "\n",
      "\n",
      " CV scores:  [ 0.87118644  0.87713311  0.87713311  0.87863248  0.8814433 ] , mean:  0.877105685977 \n",
      "\n",
      "Confusion matrix\n",
      "[[2449  102    0    0    0   12    6    0    0]\n",
      " [  22  154    0    0    0    3    0    0    0]\n",
      " [  12    4    3    0    0    1    0    0    0]\n",
      " [   7   46    0    0    0    0    0    0    0]\n",
      " [   1    0    0    0    0    0    0    0    0]\n",
      " [   8   10    0    0    0   46    0    0    0]\n",
      " [   4    0    0    0    0    1   26    0    0]\n",
      " [   3    3    0    0    0    0    0    0    0]\n",
      " [   6    0    0    0    0    0    0    0    0]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Naive Bayes with MedLine dataset**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Testing...\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.96      1.00      0.98     12033\n",
      "     B-drug       0.88      0.09      0.16       168\n",
      "     I-drug       0.00      0.00      0.00        28\n",
      "    B-brand       0.00      0.00      0.00         6\n",
      "    I-brand       0.00      0.00      0.00        90\n",
      "    B-group       0.00      0.00      0.00        58\n",
      "    I-group       0.00      0.00      0.00       114\n",
      "   B-drug_n       0.00      0.00      0.00       110\n",
      "\n",
      "avg / total       0.92      0.96      0.93     12607\n",
      "\n",
      "\n",
      " CV scores:  [ 0.95364501  0.95402299  0.95402299  0.95514093  0.95552025] , mean:  0.954470433609 \n",
      "\n",
      "Confusion matrix\n",
      "[[12031     2     0     0     0     0     0     0]\n",
      " [  153    15     0     0     0     0     0     0]\n",
      " [   28     0     0     0     0     0     0     0]\n",
      " [    6     0     0     0     0     0     0     0]\n",
      " [   90     0     0     0     0     0     0     0]\n",
      " [   58     0     0     0     0     0     0     0]\n",
      " [  114     0     0     0     0     0     0     0]\n",
      " [  110     0     0     0     0     0     0     0]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Multi Layer Perceptron with DrugBank dataset**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Testing...\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.98      0.99      0.99      2569\n",
      "     B-drug       0.92      0.80      0.86       179\n",
      "     I-drug       0.81      0.65      0.72        20\n",
      "    B-brand       0.77      0.77      0.77        53\n",
      "    I-brand       0.33      1.00      0.50         1\n",
      "    B-group       0.78      0.89      0.83        64\n",
      "    I-group       0.86      1.00      0.93        31\n",
      "   B-drug_n       0.50      0.17      0.25         6\n",
      "   I-drug_n       1.00      1.00      1.00         6\n",
      "\n",
      "avg / total       0.97      0.97      0.97      2929\n",
      "\n",
      "\n",
      " CV scores:  [ 0.91355932  0.93515358  0.90273038  0.93675214  0.91408935] , mean:  0.920456952982 \n",
      "\n",
      "Confusion matrix\n",
      "[[2539    5    1    7    2   11    4    0    0]\n",
      " [  24  143    1    5    0    4    1    1    0]\n",
      " [   6    1   13    0    0    0    0    0    0]\n",
      " [   8    4    0   41    0    0    0    0    0]\n",
      " [   0    0    0    0    1    0    0    0    0]\n",
      " [   6    0    1    0    0   57    0    0    0]\n",
      " [   0    0    0    0    0    0   31    0    0]\n",
      " [   2    2    0    0    0    1    0    1    0]\n",
      " [   0    0    0    0    0    0    0    0    6]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Multi Layer Perceptron with MedLine dataset**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Testing...\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.98      0.98      0.98     12033\n",
      "     B-drug       0.43      0.51      0.47       168\n",
      "     I-drug       0.82      0.64      0.72        28\n",
      "    B-brand       0.00      0.00      0.00         6\n",
      "    I-brand       0.57      0.39      0.46        90\n",
      "    B-group       0.83      0.83      0.83        58\n",
      "    I-group       0.24      0.25      0.25       114\n",
      "   B-drug_n       0.88      0.74      0.80       110\n",
      "\n",
      "avg / total       0.96      0.96      0.96     12607\n",
      "\n",
      "\n",
      " CV scores:  [ 0.96077655  0.96987713  0.96313912  0.96069869  0.9618745 ] , mean:  0.963273197838 \n",
      "\n",
      "Confusion matrix\n",
      "[[11842    74     4     0    22    10    72     9]\n",
      " [   65    85     0     0     2     0    16     0]\n",
      " [    8     0    18     0     0     0     0     2]\n",
      " [    3     2     0     0     1     0     0     0]\n",
      " [   40    12     0     0    35     0     3     0]\n",
      " [   10     0     0     0     0    48     0     0]\n",
      " [   59    24     0     1     1     0    29     0]\n",
      " [   29     0     0     0     0     0     0    81]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "f1_scores = []\n",
    "print(\"Training classifiers\")\n",
    "for clfName, clf in zip(names, clfs):\n",
    "    inner_f1_scores = []\n",
    "    for x_train, y_train, x_test, y_test, datasetName in zip(x_train_dic_vec, y_train_dic_vec, x_test_dic_vec, y_test_dic_vec, datasetNames):\n",
    "        printmd(\"**{} with {} dataset**\".format(clfName, datasetName))\n",
    "        print(\"Training...\")\n",
    "        clf.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Testing...\")\n",
    "        y_pred = clf.predict(x_test)\n",
    "        scores = cross_val_score(clf, x_test, y_test, cv=5)\n",
    "        inner_f1_scores.append(f1_score(y_test, y_pred, average='micro')) \n",
    "\n",
    "        print(\"Classification report\")\n",
    "        print(classification_report(y_test, y_pred, target_names=classes))\n",
    "        print(\"\\n CV scores: \", scores, \", mean: \", np.mean(scores),\"\\n\")\n",
    "\n",
    "        print(\"Confusion matrix\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        print(\"\\n\")\n",
    "    f1_scores.append(inner_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# free memory deleting unneccessary variables\n",
    "import sys\n",
    "# who\n",
    "for v in dir():\n",
    "    if not v.startswith('_') and 'dic' in v:\n",
    "        del globals()[v]\n",
    "#dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA FOR CRF MODEL\n",
      "Loading DRUGBANK train data...\n",
      "done in 71.798421s \n",
      "\n",
      "Loading DRUGBANK test data...\n",
      "done in 1.887570s \n",
      "\n",
      "train instances:  119978 test instances:  2929\n"
     ]
    }
   ],
   "source": [
    "tags = ['drug', 'brand', 'group', 'drug_n']\n",
    "\n",
    "print(\"DATA FOR CRF MODEL\")\n",
    "print(\"Loading DRUGBANK train data...\")\n",
    "t0 = time()\n",
    "drugb_x_train_arr, drugb_y_train_arr, _, _ = parse_drug_iob_xy(DRUGBANK_TRAIN_DIR, array_features, tags=tags)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "print(\"Loading DRUGBANK test data...\")\n",
    "t0 = time()\n",
    "drugb_x_test_arr, drugb_y_test_arr, drugb_sId_test, drugb_words_start = parse_drug_iob_xy(DRUGBANK_TEST_DIR, array_features, tags=tags)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "print(\"train instances: \", len(drugb_x_train_arr), \"test instances: \", len(drugb_x_test_arr))\n",
    "\n",
    "# print(\"\\n Loading MEDLINE train data...\")\n",
    "# t0 = time()\n",
    "# med_x_train_arr, med_y_train_arr, _, _ = parse_drug_iob_xy(MEDLINE_TRAIN_DIR, array_features, tags=tags)\n",
    "# duration = time() - t0\n",
    "# print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "# print(\"Loading MEDLINE test data...\")\n",
    "# t0 = time()\n",
    "# med_x_test_arr, med_y_test_arr, med_sId_test, med_words_start = parse_drug_iob_xy(MEDLINE_TEST_DIR, array_features, tags=tags)\n",
    "# duration = time() - t0\n",
    "# print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "# print(\"train instances: \", len(med_x_train_arr), \"test instances: \", len(med_x_test_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapting DRUGBANK data to numeric format\n",
      "done in 0.004995s \n",
      "\n",
      "done in 0.005996s\n"
     ]
    }
   ],
   "source": [
    "classes = ['O', 'B-drug', 'I-drug', 'B-brand', 'I-brand', 'B-group', 'I-group', 'B-drug_n', 'I-drug_n']\n",
    "\n",
    "print(\"Adapting DRUGBANK data to numeric format\"); t0 = time()\n",
    "drugb_y_test_arr_vec = vectorize_attr(drugb_y_test_arr, classes)\n",
    "duration = time() - t0; print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "# print(\"Adapting MEDLINE data to numeric format\"); t0 = time()\n",
    "# med_y_test_arr_vec = vectorize_attr(med_y_test_arr, classes)\n",
    "# duration = time() - t0; print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "# print(\"Unifying both datasets data\")\n",
    "# datasetNames = ['DrugBank', 'MedLine']\n",
    "# y_test_arr_vec = [drugb_y_test_arr_vec, med_y_test_arr_vec]\n",
    "\n",
    "# x_train_arr = [drugb_x_train_arr, med_x_train_arr]\n",
    "# y_train_arr = [drugb_y_train_arr, med_y_train_arr]\n",
    "\n",
    "# x_test_arr = [drugb_x_test_arr, med_x_test_arr]\n",
    "# y_test_arr = [drugb_y_test_arr, med_y_test_arr]\n",
    "\n",
    "duration = time() - t0; print(\"done in %fs\" % (duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**CONDITIONAL RANDOM FIELDS MODEL**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**CRF with DrugBank dataset**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training CRF file\n",
      "done in 2.652581s \n",
      "\n",
      "Training CRF\n",
      "done in 9.775827s \n",
      "\n",
      "Preparing CRF tagger\n",
      "done in 9.776824s\n",
      "Testing\n",
      "done in 0.070959s\n",
      "y_test_vec:  2929 \n",
      "\n",
      "y_pred_vec:  2929 \n",
      "\n",
      "F1 score:  0.968931375896 \n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.98      0.99      0.99      2569\n",
      "     B-drug       0.95      0.79      0.86       179\n",
      "     I-drug       0.84      0.80      0.82        20\n",
      "    B-brand       0.77      0.77      0.77        53\n",
      "    I-brand       0.50      1.00      0.67         1\n",
      "    B-group       0.88      0.81      0.85        64\n",
      "    I-group       0.91      1.00      0.95        31\n",
      "   B-drug_n       0.50      0.17      0.25         6\n",
      "   I-drug_n       1.00      1.00      1.00         6\n",
      "\n",
      "avg / total       0.97      0.97      0.97      2929\n",
      "\n",
      "Confusion matrix\n",
      "[[2549    4    1    6    1    5    3    0    0]\n",
      " [  28  141    2    5    0    2    0    1    0]\n",
      " [   4    0   16    0    0    0    0    0    0]\n",
      " [  10    2    0   41    0    0    0    0    0]\n",
      " [   0    0    0    0    1    0    0    0    0]\n",
      " [  11    0    0    1    0   52    0    0    0]\n",
      " [   0    0    0    0    0    0   31    0    0]\n",
      " [   3    2    0    0    0    0    0    1    0]\n",
      " [   0    0    0    0    0    0    0    0    6]]\n",
      "Top positive:\n",
      "8.029816 B-drug lemma=phenytoin\n",
      "7.978985 I-group prev-iob=B-group\n",
      "7.496178 I-drug_n prev-iob=B-drug_n\n",
      "7.357574 B-group lemma=quinolon\n",
      "6.888297 B-drug lemma=ethanol\n",
      "6.763875 B-group lemma=thiazid\n",
      "6.641211 B-group lemma=antacid\n",
      "6.381940 O      lemma=therapi\n",
      "6.329494 B-drug lemma=cholestyramin\n",
      "6.316242 B-brand lemma=aspirin\n",
      "5.998880 I-group prev-iob=I-group\n",
      "5.889134 B-drug lemma=ketoconazol\n",
      "5.794677 B-drug lemma=propranolol\n",
      "5.773996 B-group lemma=nsaid\n",
      "5.711710 O      lemma=caution\n",
      "5.711158 B-group lemma=cephalosporin\n",
      "5.707481 I-brand prev-iob=B-brand\n",
      "5.374704 B-drug lemma=alprazolam\n",
      "5.336145 B-group lemma=aminoglycosid\n",
      "5.312431 B-drug lemma=ketoprofen\n",
      "\n",
      "Top negative:\n",
      "-1.994356 O      lemma=anticoagul\n",
      "-1.997280 O      lemma=bronchodil\n",
      "-1.999866 O      prev-lemma=intraven\n",
      "-2.008109 O      next-lemma=oxid\n",
      "-2.137987 O      next-word=interferes\n",
      "-2.155381 O      lemma=aspirin\n",
      "-2.203537 O      prev-prev-word=amount\n",
      "-2.250745 O      next-next-word=suspension\n",
      "-2.265640 O      prev-lemma=combin\n",
      "-2.317190 O      lemma=salicyl\n",
      "-2.382253 O      prev-lemma=administ\n",
      "-2.557399 O      lemma=antihypertens\n",
      "-2.630777 O      lemma=acidifi\n",
      "-2.698204 I-group prev-pos=NNS\n",
      "-2.901191 O      lemma=anesthet\n",
      "-3.103262 O      lemma=diuret\n",
      "-3.402686 B-brand shape=lowercase\n",
      "-3.827297 I-drug prev-iob=O\n",
      "-3.978146 O      lemma=antidepress\n",
      "-4.393860 I-group prev-pos=CC\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**CRF with MedLine dataset**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training CRF file\n",
      "done in 0.814546s \n",
      "\n",
      "Training CRF\n",
      "done in 2.569054s \n",
      "\n",
      "Preparing CRF tagger\n",
      "done in 2.570055s\n",
      "Testing\n",
      "done in 0.216948s\n",
      "y_test_vec:  12607 \n",
      "\n",
      "y_pred_vec:  12607 \n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ec4924733f18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y_pred_vec: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mf1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'micro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0minner_f1_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "printmd(\"**CONDITIONAL RANDOM FIELDS MODEL**\")\n",
    "\n",
    "\n",
    "inner_f1_scores = []\n",
    "for x_train, y_train, x_test, y_test, y_test_vec, datasetName in zip(x_train_arr, y_train_arr, x_test_arr, y_test_arr, y_test_arr_vec, datasetNames):   \n",
    "    printmd(\"**{} with {} dataset**\".format('CRF', datasetName))    \n",
    "    print(\"Preparing training CRF file\"); t0 = time()  \n",
    "    trainer = pycrfsuite.Trainer(verbose=False)\n",
    "    for xseq, yseq in zip(x_train, y_train):\n",
    "        trainer.append([xseq], [yseq])\n",
    "\n",
    "    duration = time() - t0; print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "    trainer.set_params({\n",
    "        'c1': 1.0,   # coefficient for L1 penalty\n",
    "        'c2': 1e-3,  # coefficient for L2 penalty\n",
    "        'max_iterations': 50,  # stop earlier\n",
    "        # include transitions that are possible, but not observed\n",
    "        'feature.possible_transitions': True\n",
    "    })\n",
    "    trainer.params()\n",
    "\n",
    "    print(\"Training CRF\"); t0 = time()\n",
    "    trainer.train('conll2002-esp.crfsuite')\n",
    "    duration = time() - t0; print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "    print(\"Preparing CRF tagger\"); duration = time() - t0\n",
    "    tagger = pycrfsuite.Tagger(); tagger.open('conll2002-esp.crfsuite')\n",
    "    duration = time() - t0; print(\"done in %fs\" % (duration))\n",
    "\n",
    "    print(\"Testing\"); t0 = time()\n",
    "    y_pred_out = [tagger.tag([xseq]) for xseq in x_test]\n",
    "    y_pred = []\n",
    "    [y_pred.append(pred[0]) for pred in y_pred_out]\n",
    "    y_pred_vec = vectorize_attr(y_pred, classes)\n",
    "\n",
    "    duration = time() - t0; print(\"done in %fs\" % (duration))\n",
    "    \n",
    "    f1_score = f1_score(y_test_vec, y_pred_vec, average='micro')\n",
    "    inner_f1_scores.append(f1_score) \n",
    "\n",
    "    print(\"F1 score: \", f1_score,\"\\n\")\n",
    "    print(\"Classification report\"); print(classification_report(y_test_vec, y_pred_vec, target_names=classes))\n",
    "\n",
    "    print(\"Confusion matrix\"); print(confusion_matrix(y_test_vec, y_pred_vec))\n",
    "    \n",
    "    info = tagger.info()\n",
    "    print(\"Top positive:\"); print_state_features(Counter(info.state_features).most_common(20))\n",
    "    print(\"\\nTop negative:\"); print_state_features(Counter(info.state_features).most_common()[-20:])\n",
    "    \n",
    "f1_scores.append(inner_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**CONDITIONAL RANDOM FIELDS MODEL WITH DRUGBANK**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**CRF with DRUGBANK dataset**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training CRF file\n",
      "done in 0.751566s \n",
      "\n",
      "Training CRF\n",
      "done in 2.561678s \n",
      "\n",
      "Preparing CRF tagger\n",
      "done in 2.563678s\n",
      "Testing\n",
      "12607\n",
      "12607\n",
      "50428\n",
      "done in 2.062881s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [12607, 50428]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f33ff7c36f83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mduration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"done in %fs\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mduration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mf1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmed_y_test_arr_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'micro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[0minner_f1_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m    712\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[0;32m    713\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 714\u001b[1;33m                        sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m    826\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f-score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    829\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 204\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [12607, 50428]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "printmd(\"**CONDITIONAL RANDOM FIELDS MODEL WITH DRUGBANK**\")\n",
    "\n",
    "inner_f1_scores = []\n",
    "\n",
    "printmd(\"**{} with {} dataset**\".format('CRF','DRUGBANK'))    \n",
    "print(\"Preparing training CRF file\"); t0 = time()  \n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "for xseq, yseq in zip(med_x_train_arr, med_y_train_arr):\n",
    "    trainer.append([xseq], [yseq])\n",
    "\n",
    "duration = time() - t0; print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "trainer.params()\n",
    "\n",
    "print(\"Training CRF\"); t0 = time()\n",
    "trainer.train('conll2002-esp.crfsuite')\n",
    "duration = time() - t0; print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "print(\"Preparing CRF tagger\"); duration = time() - t0\n",
    "tagger = pycrfsuite.Tagger(); tagger.open('conll2002-esp.crfsuite')\n",
    "duration = time() - t0; print(\"done in %fs\" % (duration))\n",
    "\n",
    "print(\"Testing\"); t0 = time()\n",
    "print(len(med_x_test_arr))\n",
    "y_pred_outer = [tagger.tag([xseq]) for xseq in med_x_test_arr]\n",
    "print(len(y_pred_outer))\n",
    "[y_pred.append(pred[0]) for pred in y_pred_outer]\n",
    "print(len(y_pred_vec,'aqui'))\n",
    "y_pred_vec = vectorize_attr(y_pred, classes)\n",
    "print(len(y_pred_vec))\n",
    "duration = time() - t0; print(\"done in %fs\" % (duration))\n",
    "\n",
    "f1_score = f1_score(med_y_test_arr_vec, y_pred_vec, average='micro')\n",
    "inner_f1_scores.append(f1_score) \n",
    "\n",
    "print(\"F1 score: \", f1_score,\"\\n\")\n",
    "print(\"Classification report\"); print(classification_report(med_y_test_arr_vec, y_pred_vec, target_names=classes))\n",
    "\n",
    "print(\"Confusion matrix\"); print(confusion_matrix(med_y_test_arr_vec, y_pred_vec))\n",
    "\n",
    "info = tagger.info()\n",
    "print(\"Top positive:\"); print_state_features(Counter(info.state_features).most_common(20))\n",
    "print(\"\\nTop negative:\"); print_state_features(Counter(info.state_features).most_common()[-20:])\n",
    "    \n",
    "# f1_scores.append(inner_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "med_x_test_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some observations\n",
    "#### 1. Almost all top positive cases are B-drug entities, what is evident taking into account that the most entities are drugs and they are not formed by compound nouns.\n",
    "#### 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best classifier is CRF, NOT in sklearn classifiers list\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "bestIdx = [i for i, j in enumerate(f1_scores) if j == max(f1_scores)][0]\n",
    "if bestIdx < len(names):\n",
    "    print(\"The best classifier is \", names[bestIdx], \" one of the sklearn classifiers list\")\n",
    "else:\n",
    "    print(\"The best classifier is CRF, NOT in sklearn classifiers list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONLY IF best model is in sklearn classifiers list, execute the following block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-238-37c79270fc18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbestIdx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mduration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "clf = clfs[bestIdx]\n",
    "print(\"Training...\")\n",
    "t0 = time()\n",
    "clf.fit(x_train_vec, y_train_vec)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs \\n\" % (duration))\n",
    "\n",
    "print(\"Writing output TEST file\")\n",
    "t0 = time()\n",
    "words_chain = []\n",
    "with open(\"semEval_task1_output.txt\", 'w') as outfile:\n",
    "    for sId, w_start, x in zip(sId_test, words_start, x_test_dic_vec):\n",
    "        \n",
    "        prediction = clf.predict(x)\n",
    "        prediction = classes[int(prediction[0])]\n",
    "        \n",
    "        if 'B' in prediction or 'I' in prediction:\n",
    "            \n",
    "            entity = prediction.replace('B-', '').replace('I-', '')\n",
    "            word_data = vectorizer.inverse_transform(x)[0]\n",
    "            \n",
    "            for key in word_data:\n",
    "                if 'word' in str(key) and not('next' in str(key)) and not('prev' in str(key)):\n",
    "                    word = str(key).replace('word=', ''); break\n",
    "            \n",
    "            if 'B' in prediction:\n",
    "                if len(words_chain) > 0:\n",
    "                    outfile.write(\"{}|{}|{}|{}\\n\".format(words_chain[0], words_chain[1], words_chain[2], words_chain[3]))\n",
    "                    words_chain = []\n",
    "                  \n",
    "                words_chain.append(sId)\n",
    "                words_chain.append('{}-{}'.format(w_start, w_start + len(word) - 1))\n",
    "                words_chain.append(word)\n",
    "                words_chain.append(entity)\n",
    "                \n",
    "            else:\n",
    "                if len(words_chain) > 0:\n",
    "                    if words_chain[0] == sId: # if entity has more than 1 word                \n",
    "                        words_chain[1] = '{};{}-{}'.format(words_chain[1], w_start,w_start + len(word))\n",
    "                        words_chain[2] = '{} {}'.format(words_chain[2], word)\n",
    "                        \n",
    "                    else:  # if by mistake a 'I-{*}' arises and we had accumulated 'B-{*}' or 'I-{*}' from last sentence\n",
    "                        outfile.write(\"{}|{}|{}|{}\\n\".format(words_chain[0], words_chain[1], words_chain[2], words_chain[3]))\n",
    "                        words_chain = []\n",
    "                        \n",
    "                        words_chain.append(sId)\n",
    "                        words_chain.append('{}-{}'.format(w_start, w_start + len(word) - 1))\n",
    "                        words_chain.append(word)\n",
    "                        words_chain.append(entity)\n",
    "                      \n",
    "                else:  # if by mistake a 'I-{*}' arises but there is no previous 'B-{*}'\n",
    "\n",
    "                    words_chain.append(sId)\n",
    "                    words_chain.append('{}-{}'.format(w_start, w_start + len(word) - 1))\n",
    "                    words_chain.append(word)\n",
    "                    words_chain.append(entity)\n",
    "                    \n",
    "        else:\n",
    "            if len(words_chain) > 0:\n",
    "                outfile.write(\"{}|{}|{}|{}\\n\".format(words_chain[0], words_chain[1], words_chain[2], words_chain[3]))\n",
    "              \n",
    "            words_chain = []\n",
    "      \n",
    "duration = time() - t0\n",
    "print(\"done in %fs\" % (duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONLY IF best model is CRF, execute the following block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing output TEST file\n",
      "done in 0.132979s\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing output TEST file\")\n",
    "t0 = time()\n",
    "words_chain = []\n",
    "with open(\"semEval_task1_output.txt\", 'w') as outfile:\n",
    "    for sId, w_start, x in zip(sId_test, words_start, x_test_arr):\n",
    "        \n",
    "        prediction = tagger.tag([x])[0]\n",
    "        \n",
    "        if 'B' in prediction or 'I' in prediction:\n",
    "            \n",
    "            entity = prediction.replace('B-', '').replace('I-', '')\n",
    "            \n",
    "            for element in x:\n",
    "                if 'word' in element and not('next' in element) and not('prev' in element):\n",
    "                    word = element.replace('word=', ''); break\n",
    "            \n",
    "            if 'B' in prediction:\n",
    "                if len(words_chain) > 0:\n",
    "                    outfile.write(\"{}|{}|{}|{}\\n\".format(words_chain[0], words_chain[1], words_chain[2], words_chain[3]))\n",
    "                    words_chain = []\n",
    "                  \n",
    "                words_chain.append(sId)\n",
    "                words_chain.append('{}-{}'.format(w_start, w_start + len(word) - 1))\n",
    "                words_chain.append(word)\n",
    "                words_chain.append(entity)\n",
    "                \n",
    "            else:\n",
    "                if len(words_chain) > 0:\n",
    "                    if words_chain[0] == sId: # if entity has more than 1 word                \n",
    "                        words_chain[1] = '{};{}-{}'.format(words_chain[1], w_start,w_start + len(word))\n",
    "                        words_chain[2] = '{} {}'.format(words_chain[2], word)\n",
    "                        \n",
    "                    else:  # if by mistake a 'I-{*}' arises and we had accumulated 'B-{*}' or 'I-{*}' from last sentence\n",
    "                        outfile.write(\"{}|{}|{}|{}\\n\".format(words_chain[0], words_chain[1], words_chain[2], words_chain[3]))\n",
    "                        words_chain = []\n",
    "                        \n",
    "                        words_chain.append(sId)\n",
    "                        words_chain.append('{}-{}'.format(w_start, w_start + len(word) - 1))\n",
    "                        words_chain.append(word)\n",
    "                        words_chain.append(entity)\n",
    "                      \n",
    "                else:  # if by mistake a 'I-{*}' arises but there is no previous 'B-{*}'\n",
    "\n",
    "                    words_chain.append(sId)\n",
    "                    words_chain.append('{}-{}'.format(w_start, w_start + len(word) - 1))\n",
    "                    words_chain.append(word)\n",
    "                    words_chain.append(entity)\n",
    "                    \n",
    "        else:\n",
    "            if len(words_chain) > 0:\n",
    "                outfile.write(\"{}|{}|{}|{}\\n\".format(words_chain[0], words_chain[1], words_chain[2], words_chain[3]))\n",
    "              \n",
    "            words_chain = []\n",
    "      \n",
    "duration = time() - t0\n",
    "print(\"done in %fs\" % (duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
