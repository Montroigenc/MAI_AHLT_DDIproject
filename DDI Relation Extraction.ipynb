{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEPENDENCIES Python3\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk import word_tokenize, pos_tag, bigrams, trigrams\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from geniatagger import GeniaTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "# The directories where the gold standard xml is stored (update as required)\n",
    "TRAIN_DIR = './Train/'\n",
    "TEST_DIR = './Test/'\n",
    "GENIA_TAGGER_PATH = '/Users/macbook13/Desktop/geniatagger-3.0.2/geniatagger'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PARSING UTILITIES\n",
    "def is_drug_bank_filename(path):\n",
    "    return path.count('DrugBank') > 0 and path[-4:] == '.xml'\n",
    "\n",
    "\n",
    "def is_med_line_filename(path):\n",
    "    return path.count('MedLine') > 0 and path[-4:] == '.xml'\n",
    "\n",
    "\n",
    "def sentence_from_doc(doc):\n",
    "    doc = doc.replace('\\n', '').replace('</document>', '')\n",
    "    sentences = doc.split('<sentence')[1:]\n",
    "    return ['<sentence' + str for str in sentences]\n",
    "\n",
    "\n",
    "def text_from_sentence(sentence):\n",
    "    regex = r'<sentence .*?text=\"(.*?)\".*?>'\n",
    "    return re.findall(regex, sentence)[0]\n",
    "\n",
    "\n",
    "def drug_pos_from_sentence(sentence):\n",
    "    regex = r'<entity.*?id=\"(.*?)\".*?charOffset=\"(.*?)\".*?type=\"(.*?)\".*?text=\"(.*?)\"'\n",
    "    return re.findall(regex, sentence)\n",
    "\n",
    "\n",
    "def drug_pairs_from_sentence(sentence):\n",
    "    regex = r'<pair.*?e1=\"(.*?)\".*?e2=\"(.*?)\".*?(?:ddi=\".*?\" type=\"(.*?)\"\\/>|ddi=\"(.*?)\")'\n",
    "    return re.findall(regex, sentence)\n",
    "\n",
    "\n",
    "def replace_drug_name(text_string, drug_pos_string, replace_string):\n",
    "    replaced_text = []\n",
    "\n",
    "    for pos_pair in drug_pos_string.split(';'):\n",
    "        start, end = pos_pair.split('-')\n",
    "        middle = int((int(end) + int(start))/2)\n",
    "\n",
    "        for text_idx, text_char in enumerate(list(text_string)):\n",
    "            \n",
    "            if text_idx >= int(start) and text_idx <= int(end):\n",
    "                text_char = ' '\n",
    "            \n",
    "            if middle <= text_idx < middle + len(replace_string):\n",
    "                text_char = replace_string[text_idx - middle]\n",
    "                \n",
    "            replaced_text.append(text_char)                            \n",
    "\n",
    "    return ''.join(replaced_text)\n",
    "\n",
    "\n",
    "def parse_drug_ddi(xml_dir, tags=None):\n",
    "    \n",
    "    for root, dirs, files in os.walk(xml_dir):\n",
    "        \n",
    "        for file_name in files:\n",
    "            path = os.path.join(root, file_name)\n",
    "            \n",
    "            if is_drug_bank_filename(path) or is_med_line_filename(path):\n",
    "                doc = open(path, 'rb').read().decode('utf-8')\n",
    "                sentences = sentence_from_doc(doc)\n",
    "                \n",
    "                for sentence in sentences:\n",
    "                    text = text_from_sentence(sentence)\n",
    "                    drug_pos_map = {}\n",
    "                    \n",
    "                    # First we replace any drug instances with DGN token 'DGN' has been chosen as they have no conflict with the corpus\n",
    "                    for drug_id, drug_pos_str, drug_type, drug_text in drug_pos_from_sentence(sentence):\n",
    "                        drug_pos_map[drug_id] = [drug_pos_str, drug_type, drug_text]\n",
    "                        text = replace_drug_name(text, drug_pos_str, 'DGN')\n",
    "                    \n",
    "                    # Then for each pair we update the token to be either 'DGX' or 'DGY' depending on the pair\n",
    "                    for e1, e2, ddi_type, ddi_bool in drug_pairs_from_sentence(sentence):                            \n",
    "                        e1_drug_pos_str, e1_drug_type, e1_drug_text = drug_pos_map[e1]\n",
    "                        e2_drug_pos_str, e2_drug_type, e2_drug_text = drug_pos_map[e2]\n",
    "                        text = replace_drug_name(text, e1_drug_pos_str, 'DGX')\n",
    "                        text = replace_drug_name(text, e2_drug_pos_str, 'DGY')\n",
    "                        e_meta = [e1_drug_text, e1_drug_type, e2_drug_text, e2_drug_type]\n",
    "                        yield text, e_meta, str(ddi_bool or ddi_type)\n",
    "\n",
    "xml_reader = parse_drug_ddi(TRAIN_DIR)\n",
    "next(xml_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FEATURE BUILDING FUNCTIONS\n",
    "tagger = GeniaTagger(GENIA_TAGGER_PATH)\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.replace(',', ' ,').replace('-', ' ').replace('/', ' / ')\n",
    "    text = re.sub('\\d', \"num\", text)\n",
    "    text = tokenizer.tokenize(text)\n",
    "    return ' '.join(text)\n",
    "\n",
    "\n",
    "def feature_builder(data_reader):    \n",
    "    for text, e_meta, _ in data_reader:        \n",
    "        feature_dict = {}\n",
    "        sent = tokenize(text)\n",
    "        split_text = text.replace('DGX', ' DGX ').replace('DGY', ' DGY ').split()\n",
    "        e1_split = split_text.index('DGX')\n",
    "        e2_split = split_text.index('DGY')\n",
    "        sent_list, _, pos, chunk, _ = list(zip(*tagger.parse(sent)))\n",
    "        \n",
    "        #CF1 : any word between relation arguments\n",
    "        for k,i in enumerate(range(e1_split+1, e2_split)):\n",
    "            feature_dict[\"CF1_\"+str(k)] = split_text[i] \n",
    "\n",
    "        #CF2 : any pos between relation arguments\n",
    "        for k,i in enumerate(range(e1_split+1, e2_split)):\n",
    "            feature_dict[\"CF2_\"+str(k)] = split_text[i] \n",
    "\n",
    "        #CF3 : any bigram between relation arguments\n",
    "        sent_bigrams = list(bigrams(split_text[e1_split+1:e2_split]))\n",
    "        for k,bigram in enumerate(sent_bigrams):\n",
    "            feature_dict['CF3_'+str(k)] = '-'.join(bigram)\n",
    "\n",
    "        #CF4 : word preciding first argument\n",
    "        if e1_split == 0:\n",
    "            feature_dict['CF4'] = '<S>'\n",
    "        else:\n",
    "            feature_dict['CF4'] = split_text[e1_split - 1]\n",
    "\n",
    "        #CF5 : word prediding second arguments\n",
    "        if e2_split == 0:\n",
    "            feature_dict['CF5'] = '<S>'\n",
    "        else:\n",
    "            feature_dict['CF5'] = split_text[e2_split - 1]\n",
    "\n",
    "        #CF6 : any three words succeeding the first arguments\n",
    "\n",
    "        if e1_split <= len(sent) - 3 : \n",
    "            sent_trigrams = list(trigrams(split_text[e1_split+1:]))\n",
    "            for k,trigram in enumerate(sent_trigrams):\n",
    "                feature_dict['CF6_'+str(k)] = '-'.join(trigram)\n",
    "        else:\n",
    "            feature_dict['CF6_0'] = '<E>'\n",
    "\n",
    "        #CF7 : any three succeeding the second arguments\n",
    "        if e2_split <= len(sent) - 3 : \n",
    "            sent_trigrams = list(trigrams(split_text[e2_split+1:]))\n",
    "            for k,trigram in enumerate(sent_trigrams):\n",
    "                feature_dict['CF7_'+str(k)] = '-'.join(trigram)\n",
    "        else:\n",
    "            feature_dict['CF7_0'] = '<E>'\n",
    "\n",
    "        #CF8 : sequence of chunk type between relation argumemts\n",
    "        feature_dict['CF8'] = '-'.join(chunk[e1_split+1:e2_split])\n",
    "\n",
    "        #CF9 : string of words between relation arguments\n",
    "        feature_dict['CF9'] = '-'.join(split_text[e1_split+1:e2_split])\n",
    "\n",
    "        #CF13 : Distance between two arguments\n",
    "        feature_dict['CF13'] = abs(sent.index('DGX') - sent.index('DGY'))\n",
    "\n",
    "        #CF14 : Presence of puncuation sign between arguments\n",
    "        if split_text[e1_split : e2_split] == [','] or ['and'] or ['or'] or ['/'] :\n",
    "            feature_dict['CF14'] = True\n",
    "        else:\n",
    "            feature_dict['CF14'] = False\n",
    "\n",
    "        yield feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARSING DATA\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b0e40c0a0292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'false'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mechanism'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'effect'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'advise'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PARSING DATA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_drug_ddi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_drug_ddi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BUILDING FEATURES\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "classes = {'false':0, 'mechanism': 1, 'effect': 2, 'advise': 3, 'int': 4, 'true': 5}\n",
    "print(\"PARSING DATA\")\n",
    "train_data = parse_drug_ddi(TRAIN_DIR)\n",
    "test_data = parse_drug_ddi(TEST_DIR)\n",
    "print(\"BUILDING FEATURES\")\n",
    "\n",
    "if os.path.isfile('train_features.pickle'):\n",
    "    train_features = pickle.load(open('train_features.pickle', 'rb'))\n",
    "else: \n",
    "    print('Building features, may take a while')\n",
    "    train_features = feature_builder(train_data)\n",
    "\n",
    "if os.path.isfile('test_features.pickle'):\n",
    "    test_features = pickle.load(open('test_features.pickle', 'rb'))\n",
    "else: \n",
    "    print('Building features, may take a while')\n",
    "    test_features = feature_builder(test_data)\n",
    "\n",
    "print(\"VECTORIZING FEATURES\")\n",
    "vec = DictVectorizer()\n",
    "print('X_train')\n",
    "X_train = vec.fit_transform(train_features)\n",
    "print('Y_train')\n",
    "Y_train = [classes[label] for _, _, label in train_data]\n",
    "print('X_test')\n",
    "X_test = vec.transform(test_features)\n",
    "print('Y_test')\n",
    "Y_test = [classes[label] for _, _, label in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EVALUATING THE CLASSIFIER\n",
    "print(\"FITTING CLASSIFIER\")\n",
    "clf = svm.SVC(kernel='linear', C=0.1).fit(X_train, Y_train)\n",
    "a = clf.score(X_test, Y_test)\n",
    "print(\"accuracy\", a)\n",
    "y_true = Y_test\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.classification_report(y_true, y_pred,[1,2,3,4],digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
